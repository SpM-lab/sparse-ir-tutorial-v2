{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4fdc57",
   "metadata": {},
   "source": [
    "Numerical analytic continuation\n",
    "===============================\n",
    "\n",
    "One of the main problems one faces when working with imaginary (Euclidean) time is inferring the\n",
    "real-time spectra $\\rho(\\omega)$ from imaginary-time data $G(\\tau)$, in other words, we are\n",
    "seeking the inverse of the following Fredholm equation:\n",
    "\n",
    "$$\n",
    "    G(\\tau) = -\\int d\\omega\\ K(\\tau, \\omega)\\ \\rho(\\omega)\n",
    "            = -\\sum_{l=0}^\\infty U_l(\\tau)\\ S_l \\int d\\omega\\ V_l(\\omega)\\ \\rho(\\omega),\n",
    "$$\n",
    "\n",
    "where again $S_l$ are the singular values and $U_l(\\tau)$ and $V_l(\\omega)$ are the left and right\n",
    "singular (IR basis) functions, the result of a singular value expansion of the kernel $K$.\n",
    "\n",
    "Using the IR basis expansion, we can \"invert\" above equation to arrive at:\n",
    "\n",
    "$$\n",
    "    \\rho(\\omega) \\sim -\\int d\\tau\\ K^{-1}(\\omega, \\tau)\\ G(\\tau)\n",
    "    = -\\sum_{l=0}^\\infty V_l(\\omega)\\ \\frac 1{S_l} \\int d\\tau\\ U_l(\\tau)\\ G(\\tau),\n",
    "$$\n",
    "\n",
    "where $K^{-1}$ denotes the pseudoinverse of $K$.  (We will defer questions about the exact nature of this inverse.)  The numerical analytical continuation problem is now evident:  The kernel is regular, i.e., all the singular values $S_l > 0$, so the above equation can be evaluated analytically.  However, $S_l$ drop exponentially quickly, so any finite error, even simply the finite precision of $G(\\tau)$ in a computer, will be arbitrarily amplified once $l$ becomes large enough.  We say the numerical analytical continuation problem is **ill-posed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83641b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import sparse_ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79c2033",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 40.0\n",
    "wmax = 2.0\n",
    "basis = sparse_ir.FiniteTempBasis('F', beta, wmax, eps=2e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d9eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.semilogy(basis.s / basis.s[0], '-+')\n",
    "pl.title(r'singular values $s_\\ell/s_0$ of $K(\\tau, \\omega)$')\n",
    "pl.xlabel(r'index $\\ell$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393b8265",
   "metadata": {},
   "source": [
    "<!-- #region tags=[] -->\n",
    "Least squares form\n",
    "------------------\n",
    "In order to make meaningful progress, let us reformulate the analytic continutation as a least squares problem:\n",
    "\n",
    "$$\n",
    "    \\min_\\rho \\int d\\tau\\ \\Big| G(\\tau) + \\int d\\omega\\ K(\\tau, \\omega)\\ \\rho(\\omega) \\Big|^2,\n",
    "$$\n",
    "\n",
    "To simplify and speed up this equation, we want to use the IR basis form of the kernel.\n",
    "For this, remember that for any imaginary-time propagator:\n",
    "\n",
    "$$\n",
    "    G(\\tau) = \\sum_{l=0}^{L-1} g_l U_l(\\tau) + \\epsilon_L(\\tau),\n",
    "$$\n",
    "\n",
    "where the error term $\\epsilon_L$ drops as $S_L/S_0$.  We can now choose the basis cutoff $L$ large\n",
    "enough such that the error term is consistent with the intrinsic accuracy of the $G(\\tau)$, e.g.,\n",
    "machine precision.  (If there is a covariance matrix, generalized least squares should be used.)  Since the IR basis functions $U_l$ form an isometry, we also have that:\n",
    "\n",
    "$$\n",
    "    \\int\\ d\\tau\\ |G(\\tau)|^2 = \\sum_{l=0}^{L-1} |g_l|^2 + O(\\epsilon_L)\n",
    "$$\n",
    "\n",
    "allowing us to truncate our analytic continuation problem to (cf. [Jarrell and Gubernatis, 1996]):\n",
    "\n",
    "$$\n",
    "    \\min_\\rho \\sum_{l=0}^{L-1} \\Big| g_l + S_l \\int d\\omega\\ V_l(\\omega)\\ \\rho(\\omega) \\Big|^2.\n",
    "    \\qquad(1)\n",
    "$$\n",
    "\n",
    "This already is an improvement over many analytical continuation algorithms, as it maximally compresses the observed imaginary-time data $g_l$ without relying on any _a priori_ discretizations of the kernel.\n",
    "\n",
    "[Jarrell and Gubernatis, 1996]: https://doi.org/10.1016/0370-1573(95)00074-7\n",
    "<!-- #endregion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ba9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semicirc_dos(w):\n",
    "    return 2/np.pi * np.sqrt((np.abs(w) < wmax) * (1 - np.square(w/wmax)))\n",
    "\n",
    "def insulator_dos(w):\n",
    "    return semicirc_dos(8*w/wmax - 4) + semicirc_dos(8*w/wmax + 4)\n",
    "\n",
    "# For testing, compute exact coefficients g_l for two models\n",
    "rho1_l = basis.v.overlap(semicirc_dos)\n",
    "rho2_l = basis.v.overlap(insulator_dos)\n",
    "g1_l = -basis.s * rho1_l\n",
    "g2_l = -basis.s * rho2_l\n",
    "\n",
    "# Put some numerical noise on both of them (30% of basis accuracy)\n",
    "rng = np.random.RandomState(4711)\n",
    "noise = 0.3 * basis.s[-1] / basis.s[0]\n",
    "g1_l_noisy = g1_l + rng.normal(0, noise, basis.size) * np.linalg.norm(g1_l)\n",
    "g2_l_noisy = g2_l + rng.normal(0, noise, basis.size) * np.linalg.norm(g2_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd824d6",
   "metadata": {},
   "source": [
    "Truncated-SVD regularization\n",
    "----------------------------\n",
    "However, the problem is still ill-posed.  As a first attempt to cure it, let us turn to truncated-SVD regularization ([Hansen, 1987]), by demanding that the spectral function $\\rho(\\omega)$ is representable by the right singular functions:\n",
    "\n",
    "$$\n",
    "    \\rho(\\omega) = \\sum_{l=0}^{L'-1} \\rho_l V_l(\\omega),\n",
    "$$\n",
    "\n",
    "where $L' \\le L$.  The analytic continuation problem (1) then takes the following form:\n",
    "\n",
    "$$\n",
    "    \\rho_l = -g_l / S_l.\n",
    "$$\n",
    "\n",
    "The choice of $L'$ is now governed by a biasâ€“variance tradeoff: as we increase $L'$, more and more features of the spectral function emerge by virtue of $\\rho_l$, but at the same time $1/S_l$ amplifies the stastical errors more strongly.\n",
    "\n",
    "[Hansen, 1987]: https://doi.org/10.1007/BF01937276"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytic continuation made (perhaps too) easy\n",
    "rho1_l_noisy = g1_l_noisy / -basis.s\n",
    "rho2_l_noisy = g2_l_noisy / -basis.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60614cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_plot = np.linspace(-wmax, wmax, 1001)\n",
    "Vmat = basis.v(w_plot).T\n",
    "Lprime1 = basis.size // 2\n",
    "\n",
    "def _plot_one(subplot, dos, rho_l, name):\n",
    "    pl.subplot(subplot)\n",
    "    pl.plot(w_plot, dos(w_plot), \":k\", label=\"true\")\n",
    "    pl.plot(w_plot, Vmat[:, :Lprime1] @ rho_l[:Lprime1],\n",
    "            label=f\"reconstructed ($L' = {Lprime1}$)\")\n",
    "    pl.plot(w_plot, Vmat @ rho_l, lw=1,\n",
    "            label=f\"reconstructed ($L' = L = {basis.size}$)\")\n",
    "    pl.xlabel(r\"$\\omega$\")\n",
    "    pl.title(name)\n",
    "    pl.xlim(-1.02 * wmax, 1.02 * wmax)\n",
    "    pl.ylim(-.1, 1)\n",
    "\n",
    "_plot_one(121, semicirc_dos, rho1_l_noisy, r\"semi-elliptic DOS $\\rho(\\omega)$\")\n",
    "_plot_one(122, insulator_dos, rho2_l_noisy, r\"insulating DOS $\\rho(\\omega)$\")\n",
    "pl.legend()\n",
    "pl.gca().set_yticklabels([])\n",
    "pl.tight_layout(pad=.1, w_pad=.1, h_pad=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045b42e",
   "metadata": {},
   "source": [
    "Regularization\n",
    "--------------\n",
    "Above spectra are, in a sense, the best reconstructions we can achieve without including\n",
    "any more _a priori_ information about $\\rho(\\omega)$.  However, it turns out we often know\n",
    "(or can guess at) quite a lot of properties of the spectrum:\n",
    "\n",
    " 1. the spectrum must be non-negative, $\\rho(\\omega) \\ge 0$, for one orbital,\n",
    "    and positive semi-definite, $\\rho(\\omega) \\succeq 0$, in general,\n",
    "\n",
    " 2. the spectrum must be a density: $\\int d\\omega\\ \\rho(\\omega) = 1$,\n",
    "\n",
    " 3. one may assume that the spectrum may be \"sensible\", i.e., not deviate too much from\n",
    "    a default model $\\rho_0(\\omega)$ (MAXENT) or not be too complex in structure (SpM/SOM).\n",
    "\n",
    "These constraints are often encoded into the least squares problem (1) by restricting the space of valid  solutions $\\mathcal R$ and by including a regularization term $f_\\mathrm{reg}[\\rho]$:\n",
    "\n",
    "$$\n",
    "    \\min_{\\rho\\in\\mathcal R}\\bigg[ \\sum_{l=0}^{L-1} \\Big| g_l +\n",
    "    S_l \\int d\\omega\\ V_l(\\omega)\\ \\rho(\\omega) \\Big|^2 + f_\\mathrm{reg}[\\rho] \\bigg].\n",
    "    \\qquad(2)\n",
    "$$\n",
    "\n",
    "All of these constraints act as regularizers.\n",
    "\n",
    "As a simple example, let us consider **Ridge regression** in the above problem.  We again expand the spectral function in the right singular functions with $L'=L$, but include a regularization term:\n",
    "\n",
    "$$\n",
    "    f_\\mathrm{reg}[\\rho] = \\alpha \\sum_{l=0}^{L-1} | \\rho_l |^2,\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a hyperparameter (ideally tuned to the noise level).  This term prevents $\\rho_l$ becoming too large due to noise amplification. The regularized least squares problem (2) then amounts to:\n",
    "\n",
    "$$\n",
    "    \\rho_l = -\\frac{s_l}{s^2_l + \\alpha^2} g_l\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ccc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytic continuation made (perhaps too) easy\n",
    "alpha = 100 * noise\n",
    "invsl_reg = -basis.s / (np.square(basis.s) + np.square(alpha))\n",
    "rho1_l_reg = invsl_reg * g1_l_noisy\n",
    "rho2_l_reg = invsl_reg * g2_l_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97bde96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_one(subplot, dos, rho_l, rho_l_reg, name):\n",
    "    pl.subplot(subplot)\n",
    "    pl.plot(w_plot, dos(w_plot), \":k\", label=\"true\")\n",
    "    pl.plot(w_plot, Vmat @ rho_l, lw=1, label=f\"t-SVD with $L'=L$\")\n",
    "    pl.plot(w_plot, Vmat @ rho_l_reg, label=f\"Ridge regression\")\n",
    "    pl.xlabel(r\"$\\omega$\")\n",
    "    pl.title(name)\n",
    "    pl.xlim(-1.02 * wmax, 1.02 * wmax)\n",
    "    pl.ylim(-.1, 1)\n",
    "\n",
    "\n",
    "_plot_one(121, semicirc_dos, rho1_l_noisy, rho1_l_reg, r\"semi-elliptic DOS $\\rho(\\omega)$\")\n",
    "_plot_one(122, insulator_dos, rho2_l_noisy, rho2_l_reg, r\"insulating DOS $\\rho(\\omega)$\")\n",
    "pl.legend()\n",
    "pl.gca().set_yticklabels([])\n",
    "pl.tight_layout(pad=.1, w_pad=.1, h_pad=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718d9a51",
   "metadata": {},
   "source": [
    "Real-axis basis\n",
    "---------------\n",
    "One problem we are facing when solving the _regularized_ least squares problem (2) is that the regularization might \"force\" values of $g_l$ well below the threshold $L$.  (For example, in general infinitely many $g_l$ need to conspire to ensure that the spectral function is non-negative.)  This is a problem because, unlike $g_l$, which decay quickly by virtue of $S_l$, **the expansion coefficients $\\rho_l$ are not compact** (see also [Rothkopf, 2013]):\n",
    "\n",
    "Let us illustrate the decay of $\\rho_l$ for two densities of states:\n",
    "\n",
    "  1. semielliptic (left), where the $\\rho_l$ decay roughly as $1/l$, and are thus not compact.\n",
    "\n",
    "  2. discrete set of peaks: $\\rho(\\omega) \\propto \\sum_i \\delta(\\omega - \\epsilon_i)$ (right), where\n",
    "     $\\rho_l$ does not decay at all, signalling the fact that a delta-peak cannot be represented by\n",
    "     any finite (or even infinite) expansion in the basis.\n",
    "\n",
    "[Rothkopf, 2013]: https://doi.org/10.1016/j.jcp.2012.12.023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f910605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dos3 = np.array([-0.6, -0.1, 0.1, 0.6]) * wmax\n",
    "rho3_l = basis.v(dos3).sum(1)\n",
    "g3_l = -basis.s * rho3_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85196d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_one(subplot, g_l, rho_l, title):\n",
    "    pl.subplot(subplot)\n",
    "    n = np.arange(0, g_l.size, 2)\n",
    "    pl.semilogy(n, np.abs(g_l[::2]/g_l[0]), ':+b', label=r'$|G_\\ell/G_0|$')\n",
    "    pl.semilogy(n, np.abs(rho_l[::2]/rho_l[0]), ':xr', label=r'$|\\rho_\\ell/\\rho_0|$')\n",
    "    pl.title(title)\n",
    "    pl.xlabel('$\\ell$')\n",
    "    pl.ylim(1e-5, 2)\n",
    "\n",
    "_plot_one(121, g1_l, rho1_l, r'semielliptic $\\rho(\\omega)$')\n",
    "pl.legend()\n",
    "_plot_one(122, g3_l, rho3_l, r'discrete $\\rho(\\omega)$')\n",
    "pl.gca().set_yticklabels([])\n",
    "pl.tight_layout(pad=.1, w_pad=.1, h_pad=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d7e8b5",
   "metadata": {},
   "source": [
    "Thus, we need another representation for the real-frequency axis.  The simplest one is to choose a\n",
    "grid $\\{\\omega_1,\\ldots,\\omega_M\\}$ of frequencies and a function $f(x)$ and expand:\n",
    "\n",
    "$$\n",
    "    \\rho(\\omega) = \\sum_{m=1}^M a_i f(\\omega - \\omega_i),\n",
    "$$\n",
    "\n",
    "where $a_i$ are now the expansion coefficients. (More advanced methods also optimize over $\\omega_m$ and/or add some shape parameter of $f$ to the optimization parameters.)\n",
    "\n",
    "It is useful to use some probability distribution as $f(x)$, as this allows one to translate non-negativity and norm of $\\rho(\\omega)$ to non-negativity and norm of $a_i$. Since one can only observe \"broadened\" spectra in experiment for any given temperature, a natural choice is the Lorentz (Cauchy) distribution:\n",
    "\n",
    "$$\n",
    "    f(\\omega) = \\frac1\\pi \\frac\\eta{\\omega^2 + \\eta^2},\n",
    "$$\n",
    "\n",
    "where $0\\le \\eta < \\pi T$ is the \"sharpness\" parameter.  The limit $\\eta\\to 0$ corresponds to a \"hard\"  discretization using a set of delta peaks, which should be avoided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce41e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sp_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332599a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = sp_stats.cauchy(scale=0.1 * np.pi / beta).pdf\n",
    "pl.plot(w_plot, f(w_plot))\n",
    "pl.title(\"Cauchy distribution\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c047997",
   "metadata": {},
   "source": [
    "Using this discretization, we finally arrive at a form of the analytic continuation problem suitable\n",
    "for a optimizer:\n",
    "\n",
    "$$\n",
    "    \\min_{a\\in\\mathcal A}\\bigg[ \\sum_{l=0}^{L-1} \\Big| g_l - \\sum_{m=1}^M K_{lm} a_m \\Big|^2 + f_\\mathrm{reg}[\\rho[a]] \\bigg]\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "   K_{lm} := -S_l \\int d\\omega\\ V_l(\\omega)\\ f(\\omega - \\omega_m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089599f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linspace(-wmax, wmax, 21)\n",
    "K = -basis.s[:, None] * np.array(\n",
    "        [basis.v.overlap(lambda w: f(w - wi)) for wi in w]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322bf28",
   "metadata": {},
   "source": [
    "**Next**, we will examine different regularization techniques that build on the concepts in this section..."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
